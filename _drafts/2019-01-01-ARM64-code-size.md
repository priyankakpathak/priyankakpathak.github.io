---
layout: post
title: ARM64 code size
subtitle: Why it is larger than x64?
tags: [arm64, performance, assembly, work]
comments: true
---

This is the 5th of the blog posts series that talks about ARM64 performance investigation for .NET 5. You can read previous blogs at:
*  [Part 1 - ARM64 performance of .Net Core](..\2020-06-30-Dotnet-Arm64-Performance)
*  [Part 2 - Memory barriers in ARM64](..\2020-07-02-ARM64-Memory-Barriers)
*  [Part 3 - Peephole optimizations in ARM64](..\2020-07-05-ARM64-peephole-optimizations)
*  [Part 4 - Two mystic ARM64 instructions](..\2019-01-01-Two-mystic-ARM64-instructions)

In this post, I will talk about the generated code size differences that I noticed between x64 and ARM64 and what I learned from it. 

## Code size ratio of ARM64 / x64

The analysis mentioned in this blog was done by running [crossgen tool](https://github.com/dotnet/coreclr/blob/master/Documentation/building/crossgen.md) on all .NET framework libraries and saving the JIT code generated for those libraries. The JIT code was generated by setting environment variable `COMPlus_NgenDisasm=*` for both x64 and ARM64. To my surprise, the code size ratio of ARM64 / x64 was approxiametely **1.75**. That was a huge difference and a hard task for me to figure out why that is the case. There are 1000s of methods in .NET framework libraries and it is extremely difficult and nearly impossible to compare x64 vs. ARM64 generated code of every method and do the analysis. Additionally, the generated file containing JIT code of all those methods was `900MB` long.

I broke down the problem in pieces and decided to start investigating the methods having smaller x64 JIT code size but bigger ratio compared to ARM64 JIT code size. For example, imagine 2 methods `A` and `B` with following code size.

| Method Name | ARM64 code size | x64 code size | Ratio |
|-------------|-----------------|---------------|-------|
| A           | 5000 bytes      | 1000 bytes    | 5     |
| B           | 400 bytes       | 100 bytes     | 4     |

I decided to start investigating `B` although the ratio was lesser than that of `A`. However the analysis time needed for method `B` will be far lesser than that needed for `A` because there are fewer instructions in `B` to look and compare against x64.


## Code size analysis result

Below I will highlight some of the key factors that contributed to bigger size of ARM64 generated code.

### RISC vs. CISC

Since ARM64 has an ISA with fixed 32-bit instruction width, the move instructions have space for 16-bit unsigned immediate. To move bigger immediate value, we need to move the value in multiple steps using chunks of 16-bits (`movz/movk`). Due to this, multiple `mov` instructions are generated to load a single bigger value in register in contrast to x64 where a single `mov` can load bigger immediate. [This page](https://dinfuehr.github.io/blog/encoding-of-immediate-values-on-aarch64/) gives a great explanation.

I explored various options to optimize these instructions like feasibility of storing such immediate values in a literal pool and loading the values from the pool using single `ldr` instruction. However this would have involved memory operation which is expensive than 2-3 `mov` instructions. In fact, clang and gcc too use `movz/movk` and can't run away from it. More details on literal pools can be read [here](http://www.keil.com/support/man/docs/armasm/armasm_dom1359731147760.htm).

However, there are few things that we can do better. If we are trying to load same constant again and again, we can store it once in register and reuse the register value when that constant is needed. There were cases where we try to move 2 immediates back to back that are just bytes apart from each other. E.g. Below is the code generated for [Vector4.Add(Vector4, Vector4)](https://docs.microsoft.com/en-us/dotnet/api/system.numerics.vector4.add?view=netframework-4.8#System_Numerics_Vector4_Add_System_Numerics_Vector4_System_Numerics_Vector4_) method where it tries to load the parameters before performing `fadd`. 

{% highlight asm linenos %}
D29D9900          movz    x0, #0xecc8
F2A785E0          movk    x0, #0x3c2f LSL #16
F2C051C0          movk    x0, #654 LSL #32
F9400000          ldr     x0, [x0]            ; <==== loads from address 0x6543c2fecc8
FD400410          ldr     d16, [x0,#8]
D29D9800          movz    x0, #0xecc0
F2A785E0          movk    x0, #0x3c2f LSL #16
F2C051C0          movk    x0, #654 LSL #32    ; <==== loads from address 0x6543c2fecc0
F9400000          ldr     x0, [x0]
FD400411          ldr     d17, [x0,#8]
0E31D610          fadd    v16.2s, v16.2s, v17.2s
{% endhighlight %}

The 2nd set of move instructions can be optimized by doing `sub x0, x0, 8`.

Just to get the stats to see how often .NET generated these `movz/movk` instructions in framework libraries, I ran [AnalyzeAsm](https://github.com/dotnet/jitutils/tree/master/src/AnalyzeAsm) on JIT dump I created. There were total 191028 methods crossgened out of which 4578 methods contained pair of movz/movk. In all there were 11856 groups of movz/movk instructions.
You can see the summary in [movs.txt](https://github.com/dotnet/runtime/files/4453526/movs.txt). The numbers revealed that it was indeed lot of usage and we should consider optimizing it. We worked on it and decided to eliminate the loading of repeated immediate and nearby values using register instead. The changes for that work can be seen [here](https://github.com/dotnet/runtime/pull/39096). It gave **1%** improvement in JIT code size which was a great benefit in JIT world.

### Mystic adrp/add instructions

As I mentioned in my [previous blog](..\2019-01-01-Two-mystic-ARM64-instructions), we generated six instructions to load and invoke the target of a method call for `crossgen` scenarios. In those six instructions, two address load instructions `adrp` and `add` were redundant and were safe to eliminate. They formed roughly 14% of the entire JIT dump I collected for .NET framework libraries. Addressing those problems [here](https://github.com/dotnet/runtime/pull/35675) and [here](https://github.com/dotnet/runtime/pull/36817), the JIT code size of our framework libararies for `crossgen` scenarios improved by **14%**.


### Return address hijacking

From the JIT dump, I found out that there were many empty methods in framework libraries for which there was just a single instruction generated for x64 but four instructions generated for ARM64. For example, consider `System.Runtime.Versioning.NonVersionableAttribute:.ctor()` which has [an empty method body](https://source.dot.net/#System.Collections.Immutable/NonVersionableAttribute.cs,28).

The code generated for x64 is:

{% highlight asm linenos %}
; Assembly listing for method System.Runtime.Versioning.NonVersionableAttribute:.ctor():this
; Emitting BLENDED_CODE for X64 CPU with SSE2 - Windows
G_M7607_IG01:
                                          ;; bbWeight=1    PerfScore 0.00
G_M7607_IG02:
       C3                   ret
                                          ;; bbWeight=1    PerfScore 1.00
{% endhighlight %}

While the code generated for ARM64 is:

{% highlight asm linenos %}
; Assembly listing for method System.Runtime.Versioning.NonVersionableAttribute:.ctor():this
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
G_M7607_IG01:
        A9BF7BFD          stp     fp, lr, [sp,#-16]!
        910003FD          mov     fp, sp
                                          ;; bbWeight=1    PerfScore 1.50
G_M7607_IG02:
        A8C17BFD          ldp     fp, lr, [sp],#16
        D65F03C0          ret     lr
                                          ;; bbWeight=1    PerfScore 2.00
{% endhighlight %}

Why would we even generate any code for empty method body? These methods would get inlined during JIT anyway, but for `crossgen` we were creating the code for method. x64 just generated `ret`, a single instruction. But for ARM64, why is there a need of storing and loading frame pointer and link register on stack? Well, turns out that for ARM64, we do something called [return address hijacking](https://github.com/dotnet/runtime/blob/d76ef042f8ead9d06a447ab2b1004ae626185ca2/src/coreclr/src/jit/codegencommon.cpp#L4886). If .NET runtime needs to trigger a Garbage Collection (GC), it need to bring the user code to safe, special location where it can suspend the execution of user code and start the GC. For ARM64, the way it is been done is by generating code in user code's method prologue to store the return address on the stack and retrieve it from the stack in the epilogue before returning from the method. If runtime decides to trigger GC while executing the user code, it modifies the return address that is on the stack of that method to point to a special runtime helper location. When user method completes, it gets the return address from the stack into `lr` (line `8` in ARM64 code above) and return to that address (line `9`). However, since runtime updated it to special location, the execution jumps to that location where runtime triggers GC and when GC completes, jump to the original return address that was on the stack. For x64 this is not needed because return address can be retrieved from the register itself. We call this "return address hijacking" because the original return address is hijacked (during GC) and is replaced by something else. Because of this reason, .NET generates at least two instructions in method's prologue (line `4` and `5`) and remaining two in the epilogue (line `8` and `9`) which constitutes 16 bytes of code size for every method. There are scenarios (like for `NonVersionableAttribute`'s constructor code) where x64 can totally eliminate generating prologue and epilogue because there are no variables in method and hence nothing need to be stored on stack. But ARM64 code pays the penalty and generates those four instructions regardless. If interested, you can read more about ARM64 JIT frame layout [here](https://github.com/dotnet/runtime/blob/a21da8f6d945002bbb7cdb426c148867f60be528/docs/design/coreclr/jit/arm64-jit-frame-layout.md). We have a [tracking issue]((https://github.com/dotnet/runtime/issues/35274)) to optimize this pattern in future.

### Post index addressing mode

Consider a loop that accesses array.

{% highlight csharp linenos %}
public int Test()
{
    int[] arr = new int[10];
    int i = 0;
    while (i < 9)
    {
        arr[i] = 1;  // <---- IG04
        i++;
    }
    return 0;
}
{% endhighlight %}

Line `7` access array element at index `i` and store `1` to it. Here is the JIT code generated for x64:

{% highlight asm linenos %}
G_M27956_IG04:
       4863CA               movsxd   rcx, edx
       C744881001000000     mov      dword ptr [rax+4*rcx+16], 1
{% endhighlight %}

`rax` stores the base address of array `arr` (location of `arr[0]`). `rcx` holds the value of `i` and since the array is of type `int`, we multiply it by `4`. `rax+4*rcx` forms the address of array element at `ith` index. `16` is the offset from the address at which the actual value is stored. Simple enough. Now lets see what code got generated for ARM64:

{% highlight asm linenos %}
...
G_M8556_IG04:
        93407C22          sxtw    x2, x1
        D37EF442          lsl     x2, x2, #2
        91004042          add     x2, x2, #16
        52800023          mov     w3, #1
        B8226803          str     w3, [x0, x2]
...
{% endhighlight %}

What's going on here? We are calculating the array element address using four instructions? First, we load the value of `i` in `x2` (line `3`), then we multiply it by 4 (line `4`), then we add `16` to it (line `5`) and finally we store `w3` (the value `1`) in the array. Note that `x0` holds the base address of array `arr`. And remember, `arr[i] = 1` is being executed inside a loop. So we would execute all these instructions to calculate the array element in every single iteration. Can we do better? Yes, of course.

ARM64 has "post index" addressing mode just for this purpose. It basically auto increments the contents of register after executing it. This is what we want. After `str` (line `7`) completes, we want to auto-increment the address of array element by fixed value so we can store the value to that address in next iteration. 

{% highlight asm linenos %}
# x1 contains <<base address of arr>>+16
mov w0, 1
str w0, [x1], 4
{% endhighlight %}

Imagine `x1` contains base address of array + `16`. Line `3` then stores contents of `w0` in `x1` and once done, auto-increments content of `x1` by 4 (basically `x1++`). In next iteration, there is no need to calculate the element address anymore. `x1` already holds the address. There is a [tracking issue](https://github.com/dotnet/runtime/issues/34810) to optimize this pattern in future.

### Peephole optimizations

And finally, I have already covered peephole optimization opportunities in detail in my [previous blog post](2020-07-05-ARM64-peephole-optimizations) which once addressed can help in cutting down some of the code size of ARM64.

Namaste!