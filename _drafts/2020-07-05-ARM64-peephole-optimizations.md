---
layout: post
title: Peephole optimizations in .NET
subtitle: Possibilities to improve ARM64 code
tags: [arm64, performance, assembly, work]
comments: true
---

This is the 3rd of the blog posts series that talks about ARM64 performance investigation for .NET core. You can read previous blogs at:
*  [Part 1 - ARM64 performance of .Net Core](..\2020-06-30-Dotnet-Arm64-Performance).
*  [Part 2 - Memory barriers in ARM64](..\2020-07-02-ARM64-Memory-Barriers).

In this post, I will describe an important compiler phase "peephole optimization" and how much improvement it can bring to the generated machine code using examples of ARM64 code generated by .NET runtime.


### Introduction

[Peephole optimization](https://en.wikipedia.org/wiki/Peephole_optimization) is a classic compiler optimization done at the very end of code generation. The concept is very simple, imagine seeing the generated code through a peephole and changing the small set of instructions that you can see with better performant ones. An example would be easy to demonstrate. 

Below is the generated code for C#'s `HashSet.Contains()` [method](https://github.com/dotnet/runtime/blob/6c2f5feef38c8561f54fc2aeeab00ba95a5c9d38/src/libraries/System.Private.CoreLib/src/System/Collections/Generic/HashSet.cs#L199):

{% highlight asm linenos %}
        ...
        52800016          mov     w22, #0
        F9400A77          ldr     x23, [x19,#16]      
        F9400E78          ldr     x24, [x19,#24]      
        B5000BF8          cbnz    x24, G_M49529_IG15
        B4000174          cbz     x20, G_M49529_IG04
        AA1403E0          mov     x0, x20
        ...
{% endhighlight %}

Here, there are two `ldr` instructions that loads 64-bit values from memory `[x19 + 16]` and `[x19 + 24]` into `x23` and `x24` registers respectively. Thus after execution, `x23` will contain 8-bytes from memory location `[x19+16]` thru `[x19+23]` and `x24` will contain 8-bytes from memory location `[x19+24]` thru `[x19+31]`. If you refer the [ARM software optimization guide](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiGs9brqZPqAhX7JTQIHZvcAkAQFjAAegQIBRAC&url=https%3A%2F%2Fstatic.docs.arm.com%2Fswog307215%2Fa%2FArm_Cortex-A76_Software_Optimization_Guide.pdf&usg=AOvVaw2fSA7Vv6dOhvguevnFdP1e), the latency for `ldr` instruction is 4 cycles. So two instructions takes 8 cycles to complete the loading of 128-bit data from memory into registers. Is it possible to replace these two instructions with a cheaper instructions? `ldp` instruction on the other hand loads pair of 64-bit registers from memory. The latency of `ldp` is 4 cycles which is same as single `ldr` instruction. So above two `ldr` instructions can be replaced with a single `ldp` instruction to get the same values in registers `x23` and `x24`.

{% highlight asm linenos %}
        ...
        52800016          mov     w22, #0
        F9400A77          ldp     x23, x24, [x19,#16] # <-- replacement
        B5000BF8          cbnz    x24, G_M49529_IG15
        B4000174          cbz     x20, G_M49529_IG04
        AA1403E0          mov     x0, x20
        ...
{% endhighlight %}

In this example, the compiler, while scanning the generated code, can notice that there are two `ldr` instructions back to back such that they load the data from subsequent memory location and replace it with cheaper (in this case fewer cycles taking) `ldp` instruction. It should be cautious that if the two `ldr` don't get replaced if memory location is not consecutive. 

Such optimizations are known as peephole optimizations. Such replacements look simple and often seems to be of little value in desktop app but its impact can be dramatic in cloud apps. Imagine the code produced for your app has few hundred such pair of `ldr` and some of it gets called in a loop or hot function million times, by doing such optimization, you have saved 400 billion CPU cycles (100 * 1000,000 * 4) during the entire lifetime of the app.

### .NET and peephole

As I discussed in [strategy of performance investigation](..\2020-06-30-Dotnet-Arm64-Performance) previously, inspecting the generated code was the key to improve performance of ARM64. I started with saving ARM64 generated code of all the .NET framework libraries in a file. This was a gigantic file of 1 GB. I wrote small tool to parse this file to fetch the ARM64 code for a particular .NET framework libary method that I am interested in. With this tool, I started studying ARM64 code of various methods and that's when I noticed redundant instructions and could have been optimized using peephole optimization. RyuJIT, unfortunately at the time of writing this blog, didn't have peephole optimization phase. It would have taken man-months of work to write one in RyuJIT. So an interesting question that I was facing was how should I find out the benefit of a peephole optimization to the framework library without actually implementing it in RyuJIT? I needed this data to prioritize various peephole optimizations that can be done and will it be even worth to consider writing such optimizer in RyuJIT.

### AnalyzeAsm

Gladly, I figured out an easy way to get this data. Since I already had a single file containing ARM64 generated code for framework libraries, all I had to write were various analyzers to scan through this file to find instruction patterns for which peephole optimization can be done. I wrote [AnalyzeAsm](https://github.com/dotnet/jitutils/tree/master/src/AnalyzeAsm), a C# utility that would do this job and print out library methods and portion of instructions that can be optimized. It proved as the best utility I invested my time in because it helped me find at least 11 peephole optimization opportunities so far. 

### Peephole opportunties 

Let us walk through some of the opportunities that AnalyzeAsm tool helped to discover.

#### 1. Optimize "ldr+ldr" to "ldp"

This is the same example that I have given above. Optimize pair of `ldr` instructions with a single `ldp` instruction. If you refer [this](https://github.com/dotnet/runtime/issues/35132) and [this](https://github.com/dotnet/runtime/issues/35130) issue, you can see the output of `AnalyzeAsm` that tells portion of instructions along with framework library method names in which they are present. For this particular optimization, it found approx. 34,000 such pairs in 16,000 methods. That's a huge number and is definitely an important optimization to perform.

#### 2. Optimize "str+str" to "stp"

This optimization opportunity is similar to the one above except that instead of loading the values from memory, the instruction `str` stores value into memory. `stp` can perform the same operation on pair of registers if storing their values in subsequent memory. More details along with number of occurances of pair of `str` can be seen in [this](https://github.com/dotnet/runtime/issues/35133) and [this](https://github.com/dotnet/runtime/issues/35134).

The following code

{% highlight asm linenos %}
str     x13, [x12]
str     x14, [x12,#8]
{% endhighlight %}

can be optimized to 

{% highlight asm linenos %}
stp x13, x14, [x12]
{% endhighlight %}

#### 3. Optimize "str wzr + str wzr" with "str xzr"

 `wzr` is 4-byte zero register while `xzr` is a 8-byte zero register in ARM64. If 4-byte zero value is being stored in memory location followed by another 4-byte of zero value in subsequent 4-bytes, then the pair of such instruction can be replaced with a single store of 8-byte zero value. Details in [this](https://github.com/dotnet/runtime/issues/35136) issue.
 
 The following code
{% highlight asm linenos %}
str     wzr, [x2, #8]
str     wzr, [x2, #12]
{% endhighlight %}

can be optimized to 
{% highlight asm linenos %}
str     xzr, [x2, #8]
{% endhighlight %}
 

#### 4. Optimize redundant mov

 I noticed `mov` instruction pattern where a `mov` instruction was moving value from `reg1` to `reg2` and the next `mov` instruction was moving from `reg2` to `reg`. The second `mov` instruction is unnecessary and can be removed. Details in [this](https://github.com/dotnet/runtime/issues/35252) issue.

  The following code
{% highlight asm linenos %}
mov     x20, x2
mov     x2, x20
{% endhighlight %}

can be optimized to 
{% highlight asm linenos %}
mov     x20, x2
{% endhighlight %}
 
 Note: RyuJIT already optimizes and remove `mov` done between same registers i.e. `mov x0, x0`.

#### 5. Optimize and remove redundant load/store

Another pattern that I saw was loading a value from memory location into a register and then storing that value back from the register into same memory location. The second instruction is redundant and can be removed. Likewise if there is a store followed by a load. Details in [this](https://github.com/dotnet/runtime/issues/35613) and [this](https://github.com/dotnet/runtime/issues/35614) issue.

  The following code
{% highlight asm linenos %}
ldr     w0, [x19, #64]
str     w0, [x19, #64]
{% endhighlight %}

can be optimized to 
{% highlight asm linenos %}
ldr     w0, [x19, #64]
{% endhighlight %}

#### 6. Optimize memory loads with mov

RyuJIT rarely generates code that will load two registers from same memory location. The second load instruction can be converted to `mov` instruction which is cheaper and doesn't need memory access. Details in [this](https://github.com/dotnet/runtime/issues/35141) issue.

  The following code
{% highlight asm linenos %}
ldr     w1, [fp,#28]
ldr     w0, [fp,#28]
{% endhighlight %}

can be optimized to 
{% highlight asm linenos %}
ldr     w1, [fp,#28]
mov     w0, w1
{% endhighlight %}

### Conclusion

Peephole optimization are not just limited to these optimizations but can be broaden to achieve much more optimal code. They look simple but can have tremendous result on the generated code. Another important lessson I leared was that by writing a simple tool in small time, I was able to get concrete data points to justify if such optimizations are worth implementing in RyuJIT. Alternatively was either to spend man-months implementing this optimizations and measure the wins or ignore them, both of which were risky.

Namaste!